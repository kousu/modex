
-- implement Logical Replication in pre-9.4 postgres
--  also, this implementation is tuned to solve the specific problem of replicating to the web 
-- while 9.4 has Logical Replication, it a) conflicts with b) demands you poll (whereas CouchDB, whose filtered replication feature is what I'm trying to implement, [supports](http://couchdb.readthedocs.org/en/latest/replication/protocol.html#filter-replication) both pull ((long-)polling mode (the details of the long-polling variant are irrelevant to us; it's simply a workaround for HTTP being stupid, which we sidestep by not using HTTP at all)) and push ("streaming mode")



CREATE OR REPLACE LANGUAGE plpython2u;

-- DESIGN: abuse tables and triggers to construct event queues 
-- 
--  init_replicate(_table) sets watch() as a trigger on the given _table, if it is not already there
--   watch() formats changes as HRDJ (hobo-rowdelta json) format and writes to my_pg_replicate
--   my_pg_replicate is used as a queue; broadcast() is set as an INSTEAD OF trigger on it, stopping any data actually getting written to it; 
--   instead, broadcast() takes the HRDJ it received indirectly from watch() and copies it to all active clients; 
--  
--  on the "client" side, replicate(_table) first calls into the "server" side with init_replicate(_table), retrieving replicate_stream_id
--   then constructs a datagram (i.e. connectionless) socket to listen on, and calls into the "server" side 
--   register(replicate_stream_id, listen_addr)
--   which simply inserts that tuple into the table my_pg_replicate_{..??}

-- XXX replicate_stream_id is unclear at the moment; I know I need something like it to distinguish which feed of changes is being fed (at the moment, the only feeds are tables, but in the future we want to support distinct views, in order to be able to do /filtered/ replication).

--  
-- Much of the effort to go out of the way to use the DB itself as storage is because, presumably for security reasons, postgres runs multiple python processes (presumably the same is true for perl)--one per *client connection*. There is no way to just create global python objects and share them across all users. In particular, there's no way to create a pure python queue object for each client to listen on. However, that's what sockets are good for; plus, sockets (or, equivalently, a FIFO or a file) come with blocking built in for free (other options would require polling).
-- This restriction's probably a good thing. It also takes the complication of python-level concurrency out of the picture.

DROP TABLE IF EXISTS my_pg_replicate;
CREATE TABLE my_pg_replicate (event json);

CREATE OR REPLACE FUNCTION my_pg_replicate_broadcast() RETURNS trigger AS
$$
  """
  
  this trigger stops anything
  """
  # TODO...
  print("[my_pg_replicate_broadcast()]: broadcasting ", TD["new"])
  return "SKIP"
$$ LANGUAGE plpython2u;

CREATE TRIGGER my_pg_replicate_trigger
  BEFORE INSERT  -- We only catch INSERT here, because we mean to
  ON my_pg_replicate -- maintain the invariant that this table is empty
  FOR EACH ROW
  EXECUTE PROCEDURE my_pg_replicate_broadcast();



-- "-_table" should eventually become "stream_id" with a spare table mapping stream_id to (table, columns, where)
DROP TABLE IF EXISTS my_pg_replicate_clients;
CREATE TABLE my_pg_replicate_clients (stream_id text PRIMARY KEY, addr text, _table text);

--TODO: this function is simple enough that it could be done in plpgsql, which should be faster
CREATE OR REPLACE FUNCTION my_pg_replicate_register(addr text, _table text) RETURNS text AS
$$
  # returns the stream id for unregistering (& etc)
  
  import uuid
  stream_id = uuid.uuid4().hex
  
  plant = plpy.prepare("insert into my_pg_replicate_clients values ($1, $2, $3)", ["text", "text", "text"])
  plpy.execute(plant, [stream_id, addr, _table]);
  
  # this function should also install triggers on _table if they are not already there
  
  return stream_id; 
$$ LANGUAGE plpython2u;

CREATE OR REPLACE FUNCTION my_pg_replicate_unregister(stream_id text) RETURNS bool AS
$$
  plant = plpy.prepare("delete from my_pg_replicate_clients where stream_id = $1", ["text"])
  plpy.execute(plant, [stream_id])
  return True; #TODO: check if the query did anything (?? does execute() return a list of affected rows or something we can look at?)
  
  # this function should also uninstall triggers if there are no listeners for _table left
$$ LANGUAGE plpython2u;



CREATE OR REPLACE FUNCTION replicate (_table text) RETURNS SETOF json AS
$$
  """
   Create an iterator returning the state of table _table as a stream of changes in row-delta format.
    returns json because a) that is convenient for our use case (db-to-web replication)
                         a.2) anyway, we aren't giving rows, we're giving row deltas 
                         b) otherwise every different table would need a different replicate function
                                
    "_table" because "table" is a reserved word in SQL.
  """
  """
  what is a sink going to be? Do I want to use a FIFO? do i want too>??
    I cannot use anything python internal, because python instances only last per client ((which is probably a good security idea, now that I think about it))
    FIFOs satisfy most of my constraints except for the multiple listeners one
    I *could* do it with another postgres table with a trigger on it:
     - things insert to the table; 
     - watch_listeners ( table, listener, position )
     - watch_data ( table, pos, data ) with (pos) autoincrementing (within a table slice)
  
    def watch(_table):
      if not trigger exists (_table):
          create sinks[_table]
        create trigger watch_${_table} after update or insert or delete on _table for each row execute procedure watch_trigger();
          # ^as soon as this happens, the trigger may allow things to get pushed to the queue
          # ..hm. its possible to miss some events, this way, unless maybe we lock every time a (though that's dangerous..)
           #. so we need to create the sink and register ourselves as a listener, *then* push the sink.
      register self as a listener to sinks[_table] (internally, causes the sink to make another queue)

     return sinks[_table]
  """
   
  
  
  # this is code that should be library code
  # but installing it such that postgres can read it
  # and without stomping on other things too badly is hard
  # so for now it is just loaded here over and over again
  class Changes:
    MTU = 2048 #maximum bytes to read per message
    
    def __init__(self, table): #TODO: support where clauses
      self._table = table
      self._stream_id = None
      
    def __enter__(self):
      # set up our listening socket
      self._sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
      self._sock.bind(tempfile.mktemp())
      
      # register ourselves with the source
      # XXX SQL injection here
      r = plpy.execute("select * from my_pg_replicate_register('%s', '%s')" % (self._sock.getsockname(), self._table))
      r = r[0]           # register() gives a scalar, which is returned to us as a 1-row list
      r = r[r.keys()[0]] #  whose row is a 1-element dictionary
      print("register() result is: ", r)
      
      self._stream_id = r
      
      return self #oops!
    
    def __exit__(self, *args):
      # unregister ourselves
      # XXX SQL injection here
      r = plpy.execute("select * from my_pg_replicate_unregister('%s')" % (self._stream_id))
      r = r[0]
      r = r[r.keys()[0]]
      print("unregister() result is: ", r)
      
      # shutdown the socket
      os.unlink(self._sock.getsockname()) # necessary because we're using unix domain sockets
      self._sock.close()
    
    def __iter__(self):
      return self
    
    def next(self):
      fread, fwrite, ferr = select.select([self._sock], [], [self._sock])  #block until some data is available
      if ferr:
        pass #XXX
      else:
        return self._sock.recv(Changes.MTU)
  
  
  # question: why is one direction of IPC going over sockets and the other not?
  #  client -> server I'm just calling stored procedures and editing DB tables
  #  server -> client I'm using sockets
  # justification: the server is centralized; while I *could* spin up a master using a DO AS in the initialization script and have clients talk to it over sockets, since it is centralized it is just as easy to; but the clients are many, and moreover they are ephemeral.
  
  # 1) get a cursor on the current query
  #plan = plpy.prepare("select * from $1", ["text"]) # use a planner object to safeguard against SQL injection #<--- ugh, but postgres disagrees with this; I guess it doesn't want the table name to be dynamic..
  #print("the plan is", plan)
  #cur = plpy.cursor(plan, [_table]);
  cur = plpy.cursor("select * from %s" % (_table,))
  print("cur=",cur);
  
  # 2) get a handle on the change stream beginning *now* (?? maybe this involves locking?)
  # ...without logical indexing, I think I need to watch..
  # this is sort of tricky
  # I need to say somethign like
  with Changes(_table) as changes: #<-- use with to get the benefits of RAII, since Changes has a listening endpoint to worry about cleaning up
    
    # 3) spool out the current state
    for row in cur:
      print("row=",row)
      delta = {"+": row} #convert row to our made up delta format
      delta = json.dumps(delta) #and then to JSON
      print("YIELDING UP SOME STUFF YO", delta)
      yield delta
  # if this was in pure plsql, this call would be "to_json(row)"
    print("STEP THREE IS FINITO")
  # 4) spin, spooling out the change stream
    
    for delta in changes:
      # we assume that the source already jsonified things for us; THIS MIGHT BE A MISTAKE
      
      print("YIELDING UP SOME STUFF YO", delta)
      yield delta
    # NOTREACHED (unless something crashes, the changes feed should be infinite)
    print("STEP FOUR IS FINITO ")
      
$$ LANGUAGE plpython2u;